{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dill\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Models/EN_Tokenizer.pkl', 'rb') as file:\n",
    "    EN = dill.load(file)\n",
    "\n",
    "with open('../Models/AR_Tokenizer.pkl', 'rb') as file:\n",
    "    AR = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_seq_pad</th>\n",
       "      <th>Arabic_seq_pad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60_10918_10298_2414_5788_8970_293_4_59_0_0_0_0...</td>\n",
       "      <td>39_35901_25960_1429_14269_41831_43883_1718_5_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60_19045_16539_22846_24304_4_59_0_0_0_0_0_0_0_...</td>\n",
       "      <td>39_97_43052_2465_6731_38_0_0_0_0_0_0_0_0_0_0_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60_23099_22846_17059_16132_21481_572_14041_112...</td>\n",
       "      <td>39_13333_28503_23295_3160_52263_9389_7284_5294...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60_7050_22917_14981_22846_14724_16132_10344_58...</td>\n",
       "      <td>39_49647_29038_48766_8307_15199_44177_8206_135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60_20709_17685_22917_24379_23075_20709_16832_3...</td>\n",
       "      <td>39_43363_44177_9463_19915_30423_28218_24833_81...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     English_seq_pad   \n",
       "0  60_10918_10298_2414_5788_8970_293_4_59_0_0_0_0...  \\\n",
       "1  60_19045_16539_22846_24304_4_59_0_0_0_0_0_0_0_...   \n",
       "2  60_23099_22846_17059_16132_21481_572_14041_112...   \n",
       "3  60_7050_22917_14981_22846_14724_16132_10344_58...   \n",
       "4  60_20709_17685_22917_24379_23075_20709_16832_3...   \n",
       "\n",
       "                                      Arabic_seq_pad  \n",
       "0  39_35901_25960_1429_14269_41831_43883_1718_5_3...  \n",
       "1  39_97_43052_2465_6731_38_0_0_0_0_0_0_0_0_0_0_0...  \n",
       "2  39_13333_28503_23295_3160_52263_9389_7284_5294...  \n",
       "3  39_49647_29038_48766_8307_15199_44177_8206_135...  \n",
       "4  39_43363_44177_9463_19915_30423_28218_24833_81...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"English_seq_pad\"] = df[\"English_seq_pad\"].apply(lambda x : np.array(x.split(\"_\")).astype(int))\n",
    "df[\"Arabic_seq_pad\"] = df[\"Arabic_seq_pad\"].apply(lambda x : np.array(x.split(\"_\")).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df[\"English_seq_pad\"].values, df[\"Arabic_seq_pad\"].values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "encoder = Encoder(EN.dictlength, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(AR.dictlength, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.tolist(), y_train.tolist())).shuffle(len(X_train)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([AR.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        total_loss += (loss / int(targ.shape[1]))\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "      \n",
    "        optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss.numpy() / int(targ.shape[1])))\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss/df.shape[0]))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('../Models/encoder_en2ar.h5')\n",
    "decoder.save_weights('../Models/decoder_en2ar.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Models/encoder_en2ar.json', 'w') as file:\n",
    "    file.write(encoder.to_json())\n",
    "\n",
    "with open('../Models/decoder_en2ar.json', 'w') as file:\n",
    "    file.write(decoder.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df[\"Arabic_seq_pad\"].values,df[\"English_seq_pad\"].values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "encoder = Encoder(AR.dictlength, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(EN.dictlength, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.tolist(), y_train.tolist())).shuffle(len(X_train)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([EN.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        total_loss += (loss / int(targ.shape[1]))\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "      \n",
    "        optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss.numpy() / int(targ.shape[1])))\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss/df.shape[0]))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('../Models/encoder_ar2en.h5')\n",
    "decoder.save_weights('../Models/decoder_ar2en.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Models/encoder_ar2en.json', 'w') as file:\n",
    "    file.write(encoder.to_json())\n",
    "\n",
    "with open('../Models/decoder_ar2en.json', 'w') as file:\n",
    "    file.write(decoder.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, encoder, decoder ,input_tok , output_tok):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence,\"en\")\n",
    "\n",
    "    inputs = input_tok.texts_to_sequences([sentence])[0]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_tok.maxlen, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([output_tok.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(output_tok.maxlen):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        predicted_id = tf.random.categorical(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
    "\n",
    "        result += output_tok.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if output_tok.idx2word[predicted_id] == '<end>':\n",
    "            return result\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(\"hello world!\", encoder, decoder, EN, AR)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
